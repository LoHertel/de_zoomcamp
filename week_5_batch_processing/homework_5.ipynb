{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Homework\n",
    "\n",
    "In this homework we'll put what we learned about Spark\n",
    "in practice.\n",
    "\n",
    "We'll use high volume for-hire vehicles (HVFHV) dataset for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local spark session \n",
    "* Execute `spark.version`\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../data'\n",
      "/Users/lorenz/repos/data_engineering_zoomcamp/week_5_batch_processing\n",
      "--2022-03-08 02:12:28--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.216.134.75\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.216.134.75|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  1.80MB/s    in 4m 20s  \n",
      "\n",
      "2022-03-08 02:16:50 (2.69 MB/s) - ‘fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd data\n",
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read it with Spark using the same schema as we did \n",
    "in the lessons. We will use this dataset for all\n",
    "the remaining questions.\n",
    "\n",
    "Repartition it to 24 partitions and save it to\n",
    "parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.BooleanType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('data/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)\n",
    "df.write.parquet('data/fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221M\tdata/fhvhv/2021/02/\n"
     ]
    }
   ],
   "source": [
    "!du -h data/fhvhv/2021/02/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size is 221 MB -> correct answer 208 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Count records \n",
    "\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .filter(F.to_date(df.pickup_datetime) == F.lit('2021-02-15')) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2021-02-11|        75540|\n",
      "| 2021-02-17|        57221|\n",
      "| 2021-02-20|        44039|\n",
      "| 2021-02-03|        40653|\n",
      "| 2021-02-19|        37577|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('duration', df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long')) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "    .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Most frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` \n",
    "in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "> Note: the answer may depend on how you write the query,\n",
    "> so there are multiple correct answers. \n",
    "> Select the one you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "|              B02875| 685390|\n",
      "|              B02765| 559768|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .groupBy('dispatching_base_num') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Spark Jobs the query had 2 stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair. \n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('data/zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values in column Zone to \"Unknown\", when the Borough is unknown. The values are 'NA' and 'NV' beforehand.\n",
    "df_zones = df_zones \\\n",
    "    .withColumn('Zone',\n",
    "        F.when(F.upper(df_zones.Borough) == 'UNKNOWN', F.lit('Unknown')) \\\n",
    "        .otherwise(df_zones.Zone)) \\\n",
    "    .toDF(*df_zones.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename dataframe and columns as a preparation to join two times afterwards\n",
    "df_zones_pu = df_zones.alias(\"df_zones_pu\") \\\n",
    "                      .withColumnRenamed('LocationID', 'pu_LocationID') \\\n",
    "                      .withColumnRenamed('Borough', 'pu_Borough') \\\n",
    "                      .withColumnRenamed('Zone', 'pu_Zone') \\\n",
    "                      .withColumnRenamed('service_zone', 'pu_service_zone')\n",
    "df_zones_do = df_zones.alias(\"df_zones_do\") \\\n",
    "                      .withColumnRenamed('LocationID', 'do_LocationID') \\\n",
    "                      .withColumnRenamed('Borough', 'do_Borough') \\\n",
    "                      .withColumnRenamed('Zone', 'do_Zone') \\\n",
    "                      .withColumnRenamed('service_zone', 'do_service_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(pd_pair='East New York / East New York', count=45041),\n",
       " Row(pd_pair='Borough Park / Borough Park', count=37329),\n",
       " Row(pd_pair='Canarsie / Canarsie', count=28026)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .join(df_zones_pu, df.PULocationID == df_zones_pu.pu_LocationID, 'left') \\\n",
    "    .join(df_zones_do, df.DOLocationID == df_zones_do.do_LocationID, 'left') \\\n",
    "    .withColumn('pd_pair', F.concat(df_zones_pu.pu_Zone, F.lit(' / '), df_zones_do.do_Zone)) \\\n",
    "    .select('pd_pair') \\\n",
    "    .groupBy('pd_pair') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question. Join type\n",
    "\n",
    "(not graded) \n",
    "\n",
    "For finding the answer to Q6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?\n",
    "\n",
    "And how many stages your spark job has?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a ``LEFT JOIN``. The spark job used 3 stages:\n",
    "1. Use broadcast join, send zones data to every partition.\n",
    "2. Join data in every partition.\n",
    "3. Reshuffle intermediate results and combine to final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the solutions\n",
    "\n",
    "* Form for submitting: https://forms.gle/dBkVK9yT8cSMDwuw7\n",
    "* You can submit your homework multiple times. In this case, only the last submission will be used. \n",
    "\n",
    "Deadline: 07 March (Monday), 22:00 CET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2d3ce0c5f16d2196f57a43042a46e95ca61a0aae8277eeefa1c73b42baeade9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
